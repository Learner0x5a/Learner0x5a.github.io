<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Learner0x5a&#39;s Studio</title>
  
  
  <link href="https://learner0x5a.github.io/atom.xml" rel="self"/>
  
  <link href="https://learner0x5a.github.io/"/>
  <updated>2021-03-19T15:40:58.021Z</updated>
  <id>https://learner0x5a.github.io/</id>
  
  <author>
    <name>Learner0x5a</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>pytorch笔记</title>
    <link href="https://learner0x5a.github.io/2021/03/19/pytorch-notes/"/>
    <id>https://learner0x5a.github.io/2021/03/19/pytorch-notes/</id>
    <published>2021-03-19T05:38:24.000Z</published>
    <updated>2021-03-19T15:40:58.021Z</updated>
    
    <content type="html"><![CDATA[<h1 id="官方教程-Learn-PyTorch-with-Examples"><a href="#官方教程-Learn-PyTorch-with-Examples" class="headerlink" title="官方教程 Learn PyTorch with Examples"></a>官方教程 Learn PyTorch with Examples</h1><p><a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">https://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a></p><h1 id="PyTorch-Batch训练的一些笔记"><a href="#PyTorch-Batch训练的一些笔记" class="headerlink" title="PyTorch Batch训练的一些笔记"></a>PyTorch Batch训练的一些笔记</h1><p>根据可视化结果，batch size越小，得到的分类边界更精细，收敛更好，但收敛越慢，batch size过小，就会导致震荡甚至无法收敛。<br>batch size越大，收敛越快，但分类边界比较粗糙。</p><h1 id="调用指定的GPU"><a href="#调用指定的GPU" class="headerlink" title="调用指定的GPU"></a>调用指定的GPU</h1><p>直接终端中设定</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1 python3 main.py</span><br></pre></td></tr></table></figure><h1 id="torch-nonzero"><a href="#torch-nonzero" class="headerlink" title="torch.nonzero"></a>torch.nonzero</h1><p>torch.nonzero() 找出tensor中非零的元素的索引.</p><h1 id="PyTorch-梯度传播的笔记"><a href="#PyTorch-梯度传播的笔记" class="headerlink" title="PyTorch 梯度传播的笔记"></a>PyTorch 梯度传播的笔记</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])  </span><br><span class="line">tensor.requires_grad=<span class="literal">True</span>  </span><br><span class="line">mean_out = torch.mean(tensor*tensor)       <span class="comment"># x^2  </span></span><br><span class="line">max_out = torch.<span class="built_in">max</span>(tensor*tensor)  </span><br><span class="line">  </span><br><span class="line">print(mean_out)  <span class="comment"># tensor(7.5000, grad_fn=&lt;MeanBackward0&gt;)  </span></span><br><span class="line">print(max_out)  <span class="comment"># tensor(16., grad_fn=&lt;MaxBackward1&gt;)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次backward会自动销毁计算图</span></span><br><span class="line">mean_out.backward()    <span class="comment"># 反向传播; 这里不用retain_graph=True，因为mean_out和max_out是不同的计算图；这里只销毁mean_out</span></span><br><span class="line">print(tensor.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0.5000, 1.0000],</span></span><br><span class="line"><span class="string">    [1.5000, 2.0000]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">max_out.backward(retain_graph=<span class="literal">True</span>) <span class="comment"># 自动累计梯度; 这里需要加retain_graph=True，因为下面还要用max_out</span></span><br><span class="line">print(tensor.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[ 0.5000,  1.0000],</span></span><br><span class="line"><span class="string">    [ 1.5000, 10.0000]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor.grad = <span class="literal">None</span> <span class="comment"># 如果想重新算梯度，需要将梯度清零；tensor.grad = None</span></span><br><span class="line">max_out.backward()</span><br><span class="line">print(tensor.grad)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">tensor([[0., 0.],</span></span><br><span class="line"><span class="string">    [0., 8.]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mean_out = 1/4 * sum(tensor*tensor)   </span></span><br><span class="line"><span class="comment"># mean_out 的梯度是d(mean_out)/d(tensor) = 1/4*2*tensor = tensor/2  </span></span><br><span class="line"><span class="comment"># max_out = max(tensor*tensor)   </span></span><br><span class="line"><span class="comment"># max_out 的梯度是d(max_out)/d(tensor) = max(2*tensor)  </span></span><br><span class="line"><span class="comment"># mean,max之类的函数不参与梯度运算，原样保留  </span></span><br><span class="line"><span class="comment"># max simply selects the greatest value and ignores the others, so max is the identity operation for that one element.   </span></span><br><span class="line"><span class="comment"># Therefore the gradient can flow backwards through it for just that one element.  </span></span><br><span class="line">  </span><br><span class="line">  </span><br></pre></td></tr></table></figure><h2 id="NN-花式求梯度"><a href="#NN-花式求梯度" class="headerlink" title="NN 花式求梯度"></a>NN 花式求梯度</h2><p>NN来自<a href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py">https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fully connected neural network with one hidden layer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc2 = nn.Linear(hidden_size, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.fc1(x)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个函数用来算梯度，这里其实和forward一样，用来算输出的梯度; </span></span><br><span class="line">    <span class="comment"># 也可以改成别的，比如去掉fc2和relu，算一下第一层对输入的梯度；</span></span><br><span class="line">    <span class="comment"># 也可以指定某个神经元的梯度，比如return out[:,2]就是只用第三个神经元算梯度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_grad</span>(<span class="params">self,x</span>):</span>     </span><br><span class="line">        out = self.fc1(x)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">model = NeuralNet(input_size, hidden_size, num_classes).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loss and optimizer</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, (images, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="comment"># Move tensors to the configured device</span></span><br><span class="line">        images = images.reshape(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>).to(device)</span><br><span class="line">        images.requires_grad = <span class="literal">True</span></span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward pass</span></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward and optimize</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 这里计算的是loss对input的梯度; shape (100,784); loss是一个标量，不需要传参给backward()</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        print(np.shape(images.grad),images.grad[<span class="number">0</span>][:<span class="number">5</span>])</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算输出对输入的梯度 d(output) / d(input); shape也是(100,784)，原因参见下面backward的解析</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        out = model.pre_grad(images)</span><br><span class="line">        <span class="comment"># 求向量对向量的梯度，需要传一个grad_tensor进去，这个grad_tensor和out的shape要一样</span></span><br><span class="line">        <span class="comment"># 这里的ones_like(out)就是产生一个和out的shape一样，元素全为1的tensor</span></span><br><span class="line">        <span class="comment"># 这个grad_tensor的物理意义是权重; </span></span><br><span class="line">        <span class="comment"># torch在算完Jacobian矩阵后，会用这个grad_tensor乘上Jacobian矩阵，对各行（即各个神经元对input的梯度）做一个加权和</span></span><br><span class="line">        <span class="comment"># 所以得到的仍然是一个shape为(100,784)的张量</span></span><br><span class="line">        <span class="comment"># 可以参考：https://blog.csdn.net/weixin_38314865/article/details/100423919</span></span><br><span class="line">        out.backward(torch.ones_like(out)) </span><br><span class="line">        print(np.shape(images.grad),images.grad[<span class="number">0</span>][:<span class="number">5</span>])</span><br><span class="line">        images.grad = <span class="literal">None</span> <span class="comment"># 记得清零</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">            # 如果想要得到原始的Jacobian矩阵，需要利用one-hot向量多次backward，再把结果拼起来; 例如</span></span><br><span class="line"><span class="string">            x = torch.randn(2, requires_grad=True)</span></span><br><span class="line"><span class="string">            y = x * 2</span></span><br><span class="line"><span class="string">            J = torch.zeros(x.shape[0],x.shape[0]) # 初始化空Jacobian矩阵 (2*2)</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            y.backward(torch.FloatTensor([[1,0]]),retain_graph=True) # 第一行</span></span><br><span class="line"><span class="string">            J[:,i] = x.grad</span></span><br><span class="line"><span class="string">            x.grad = None</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            y.backward(torch.FloatTensor([[0,1]]),retain_graph=True) # 第二行</span></span><br><span class="line"><span class="string">            J[:,i] = x.grad</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            print(J)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            # 对于这个nn，Jacobian矩阵也可以通过修改计算图pre_grad，遍历神经元再拼起来得到</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">计算单个样本的梯度</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">        <span class="comment"># 也可以计算对单个样本的梯度; shape (1,784)</span></span><br><span class="line">        img = images[<span class="number">0</span>].view(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line">        img.requires_grad = <span class="literal">True</span> <span class="comment"># 注意，计算单个样本的时候，需要把上面对整体样本的设置注视掉，即注释掉images.requires_grad = True</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        out = model.pre_grad(img)</span><br><span class="line">        out.backward(torch.ones_like(out)) <span class="comment"># 同上，传入Jacobian矩阵加权向量</span></span><br><span class="line">        print(np.shape(img.grad),img.grad[:,:<span class="number">5</span>])</span><br><span class="line">        img.grad = <span class="literal">None</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;官方教程-Learn-PyTorch-with-Examples&quot;&gt;&lt;a href=&quot;#官方教程-Learn-PyTorch-with-Examples&quot; class=&quot;headerlink&quot; title=&quot;官方教程 Learn PyTorch with Exam</summary>
      
    
    
    
    
    <category term="AI" scheme="https://learner0x5a.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>实用网站收集</title>
    <link href="https://learner0x5a.github.io/2021/02/07/websitecollection/"/>
    <id>https://learner0x5a.github.io/2021/02/07/websitecollection/</id>
    <published>2021-02-07T03:20:45.000Z</published>
    <updated>2021-02-07T03:27:04.925Z</updated>
    
    <content type="html"><![CDATA[<h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><p><a href="https://www.deepl.com/translator">deepl</a></p><h1 id="IDA"><a href="#IDA" class="headerlink" title="IDA"></a>IDA</h1><p><a href="https://magiclantern.fandom.com/wiki/IDAPython">IDA Python wiki</a><br><a href="http://www.idabook.com/">IDA Book</a></p><h1 id="社工"><a href="#社工" class="headerlink" title="社工"></a>社工</h1><p><a href="https://haveibeenpwned.com/">密码撞库检查</a></p><h1 id="黑苹果"><a href="#黑苹果" class="headerlink" title="黑苹果"></a>黑苹果</h1><p><a href="http://blog.daliansky.net/">黑果小兵</a></p><h1 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h1><p><a href="https://godbolt.org/">Compiler Explorer</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;翻译&quot;&gt;&lt;a href=&quot;#翻译&quot; class=&quot;headerlink&quot; title=&quot;翻译&quot;&gt;&lt;/a&gt;翻译&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.deepl.com/translator&quot;&gt;deepl&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;IDA&quot;&gt;</summary>
      
    
    
    
    
    <category term="Tools" scheme="https://learner0x5a.github.io/tags/Tools/"/>
    
  </entry>
  
  <entry>
    <title>以孝治国：孝与家国伦理</title>
    <link href="https://learner0x5a.github.io/2021/02/01/yxzg/"/>
    <id>https://learner0x5a.github.io/2021/02/01/yxzg/</id>
    <published>2021-02-01T14:59:47.000Z</published>
    <updated>2021-02-01T15:51:59.867Z</updated>
    
    <content type="html"><![CDATA[<p>夫孝者，善继人之志，善述人之志。 —— 《礼记·中庸》</p><h1 id="孝与孝道"><a href="#孝与孝道" class="headerlink" title="孝与孝道"></a>孝与孝道</h1><p>孝是子女对父母的自然亲情。<br>殷商时代，孝是宗教性的。以崇拜，祭祀祖先为根本目的，祈求祖先神灵庇佑。</p><p>孝道是被儒学家和统治者升华，外延，扭曲，推向极端的封建伦理道德。<br>但在中国社会中二者并未截然分开。</p><p>孝具有政治外延性。移孝为忠，报效国家和民族，不仅要孝敬老人，还要尊重长辈的意愿，实现长辈的愿望，为家族，乡里，国家争得荣誉。<br>此即成为统治思想中的孝道。</p><h1 id="儒家的孝道"><a href="#儒家的孝道" class="headerlink" title="儒家的孝道"></a>儒家的孝道</h1><p>为了维护父家长传统的等级制，孔子强调“孝悌”“亲亲尊尊”。<br>孔子将孝作为人格修养的根本，外推到亲族，社会和国家政治。<br>经由历代儒学家层层加码，孝演变成为治国的伦理工具。<br>将社会政治和家庭伦理结合，把外在的等级制度内化为每个人必须具备的伦理道德意识和自觉要求。</p><p>儒家提倡不违父母之命。谏诤不从，只得服从。“三谏而不听则号泣而随之。”<br>为了“亲亲尊尊”，采用“子为父隐”掩盖事实真相，维护尊长颜面。</p><p>孔子是儒家不是法家，在儒家这里，只有礼，没有法。</p><p>古代中国人的经世观念讲究立身扬名，不辱没祖先，否则为不孝。这一观念由《三字经》贯彻到世俗社会——“扬名声，显父母”。</p><p>汉朝开始，孝道被扭曲，强化，走向极端。“举孝廉”时期，为了博取孝子的美名，正常的孝被视为平淡，必须变本加厉，超越礼制，孝出个高水平，高难度，才能引起社会和朝廷的注意。由此形成愚孝，假孝，欺世盗名，惊世骇俗。<br><em>“生不养，死厚葬。”</em></p><p>随着孝道的强化，开始强调“树欲静而风不止，子欲养而亲不待”。个人必须放弃自身价值的实现，将“尽孝”放在首位。<br>“无违”变为“父叫子亡，子不亡不孝”，“棍棒底下出孝子”。<br>是非判断的落点归于孝：人不孝敬父母，禽兽不如，天地不容。<br>孝从人情变为了天理。</p><h1 id="佛教"><a href="#佛教" class="headerlink" title="佛教"></a>佛教</h1><p>佛教视人生为苦海，要求人们四大皆空，六根清净，了却生死，超脱世俗。<br>儒家是奋力入世的哲学，让人生价值实现在今生今世。<br>佛教是消极避世的哲学，让人生价值实现在来世，不重视今生今世的人际伦理，<em>不跪王者，不敬父母</em>。</p><h1 id="以孝齐家"><a href="#以孝齐家" class="headerlink" title="以孝齐家"></a>以孝齐家</h1><p>从先秦儒家经典到程朱理学，文人政客对家庭伦理你讲五品，我说六顺，他言七教，然后又有八德，十礼，致使家庭伦理的繁文缛节层层累积，令人目不暇接。<br>《礼记·礼运篇》提出“十义”：父慈子孝，兄良弟悌，夫义妇听，长惠幼顺，君仁臣忠。</p><p>宗法社会强调父子，兄弟，夫妇，长幼双方等家庭伦理，这些建立在道德等价交换和“平等”人格的基础上。<br>三纲五常之后，家庭伦理开始失衡。弱化君仁，父慈，夫义，强调臣忠，子孝，妇听。</p><h1 id="以孝治国"><a href="#以孝治国" class="headerlink" title="以孝治国"></a>以孝治国</h1><p>历代王朝通过“官”统治人们，美其名曰“民之父母”，把被统治者亲切地称之为“子民”，把统治者称为“父母官”。<br>被统治者需要像儿子对待父亲一样对待父母官和君父。</p><p>对君父和父母官对崇拜和拥戴，使得农民把实现和谐生活对幻想寄托在有道明君和青天大老爷身上。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;夫孝者，善继人之志，善述人之志。 —— 《礼记·中庸》&lt;/p&gt;
&lt;h1 id=&quot;孝与孝道&quot;&gt;&lt;a href=&quot;#孝与孝道&quot; class=&quot;headerlink&quot; title=&quot;孝与孝道&quot;&gt;&lt;/a&gt;孝与孝道&lt;/h1&gt;&lt;p&gt;孝是子女对父母的自然亲情。&lt;br&gt;殷商时代，孝是宗教性</summary>
      
    
    
    
    
    <category term="BookReading" scheme="https://learner0x5a.github.io/tags/BookReading/"/>
    
  </entry>
  
  <entry>
    <title>IDA Linux 配置</title>
    <link href="https://learner0x5a.github.io/2021/01/18/IDALinux/"/>
    <id>https://learner0x5a.github.io/2021/01/18/IDALinux/</id>
    <published>2021-01-18T14:42:07.000Z</published>
    <updated>2021-01-25T15:33:06.696Z</updated>
    
    <content type="html"><![CDATA[<h1 id="IDA-Linux-Conf"><a href="#IDA-Linux-Conf" class="headerlink" title="IDA-Linux-Conf"></a>IDA-Linux-Conf</h1><p>Configure IDA pro linux version to support custom python/pip. 配置linux版IDA，支持自己的python/pip。</p><p>Tested with IDA pro 6.8 on Ubuntu 18.04 64bit.</p><h2 id="32位环境配置；32bit-enviroment"><a href="#32位环境配置；32bit-enviroment" class="headerlink" title="32位环境配置；32bit enviroment"></a>32位环境配置；32bit enviroment</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dpkg --add-architecture i386</span><br><span class="line">apt update</span><br><span class="line">apt dist-upgrade</span><br><span class="line">apt install gcc-multilib g++-multilib dialog apt-utils</span><br><span class="line">apt install lib32ncurses5 lib32z1</span><br><span class="line">apt install lib32stdc++6 build-essential libssl-dev libffi-dev python-dev libssl-dev libgcc1:i386 zlib1g:i386</span><br></pre></td></tr></table></figure><h2 id="使用自己的python-pip；Use-custom-python-pip"><a href="#使用自己的python-pip；Use-custom-python-pip" class="headerlink" title="使用自己的python/pip；Use custom python/pip"></a>使用自己的python/pip；Use custom python/pip</h2><p>linux版本的ida pro自带的python是没有binary的，只有一个libpython.so.1.0，没有pip，不能安装第三方的模块比如capstone。</p><p>网上的解决方法（Solutions from internet）：</p><ul><li><p>没看懂的操作，但对现有方法做了<a href="https://duksctf.github.io/2017/03/15/Make-IDA-Pro-Great-Again.html">汇总</a>；A summary</p></li><li><p><a href="https://github.com/zardus/idalink">idalink</a>：看起来很好用，但要求ida版本&gt;7.0；Require IDA version &gt; 7.0</p></li></ul><ul><li><p><a href="https://pythonhosted.org/arybo/integration.html">Arybo</a>：用rpc重定向至64位的python；Redirect to 64bit python with rpc</p></li><li><p>官方<a href="https://www.hex-rays.com/blog/installing-pip-packages-and-using-them-from-ida-on-a-64-bit-machine/">博客</a>：太老了；Official solution, maybe too old.</p></li></ul><h3 id="编译一个32位的python2-7；Compile-a-32-bit-python2-7"><a href="#编译一个32位的python2-7；Compile-a-32-bit-python2-7" class="headerlink" title="编译一个32位的python2.7；Compile a 32-bit python2.7"></a>编译一个32位的python2.7；Compile a 32-bit python2.7</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apt install zlib1g-dev:i386 libsqlite3-dev:i386 libssl-dev:i386 libreadline-dev:i386 libncurses5-dev:i386 libffi-dev:i386 libbz2-dev:i386 libgcc1:i386 zlib1g:i386 libc6-dev-i386</span><br><span class="line">wget https://www.python.org/ftp/python/2.7.18/Python-2.7.18.tar.xz</span><br><span class="line">tar xvf Python-2.7.18.tar.xz</span><br><span class="line"><span class="built_in">cd</span> Python-2.7.18</span><br><span class="line">CFLAGS=-m32 LDFLAGS=-m32 ./configure --prefix=/path/to/python27-32 --with-system-ffi --with-ensurepip=install</span><br><span class="line">make -j24</span><br></pre></td></tr></table></figure><p>可能有部分模块不能编译；There can be some modules that cannot be built</p><p>没有Failed to build these modules即可。But we only need to make sure there’s no compilation failure, i.e. Failed to build these modules.</p><p>Finally, <code>make install</code></p><h3 id="使用编译得到的pip安装想要的包；Use-compiled-python-pip-to-install-whatever-you-want"><a href="#使用编译得到的pip安装想要的包；Use-compiled-python-pip-to-install-whatever-you-want" class="headerlink" title="使用编译得到的pip安装想要的包；Use compiled python-pip to install whatever you want"></a>使用编译得到的pip安装想要的包；Use compiled python-pip to install whatever you want</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CFLAGS=-m32</span><br><span class="line"><span class="built_in">export</span> LDFLAGS=-m32</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/lib/i386-linux-gnu/:/usr/lib32:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line">/path/to/python27-32/bin/pip install &lt;package&gt;</span><br></pre></td></tr></table></figure><h3 id="使用安装的包；Use-installed-packages"><a href="#使用安装的包；Use-installed-packages" class="headerlink" title="使用安装的包；Use installed packages"></a>使用安装的包；Use installed packages</h3><p>在你脚本里添加；In your python script, add</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sys.path.append(<span class="string">&quot;/path/to/python27-32/lib/python2.7/site-packages&quot;</span>) </span><br><span class="line">sys.path.append(<span class="string">&quot;/path/to/python27-32&quot;</span>) </span><br><span class="line">sys.path.append(<span class="string">&quot;/path/to/python27-32&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="Bug-code-for-hash-md5-was-not-found"><a href="#Bug-code-for-hash-md5-was-not-found" class="headerlink" title="Bug: code for hash md5 was not found"></a>Bug: code for hash md5 was not found</h2><p>ldd查看IDA PRO自带的Python2.7 lib中_hashlib.so，提示缺少libssl.so.0.9.8 libcrypto.so.0.9.8 libpython2.7.so.1.0。</p><p>其中libpython2.7.so.1.0在IDA PRO安装目录下，将其拷贝至对应库目录下即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;path&#x2F;to&#x2F;ida&#x2F;libpython2.7.so.1.0 &#x2F;path&#x2F;to&#x2F;ida&#x2F;python&#x2F;lib&#x2F;python2.7&#x2F;lib-dynload&#x2F;</span><br></pre></td></tr></table></figure><p>IDA PRO 6.8自带的python2.7要求openssl版本为32bit 0.9.8，并且openssl1.0.0和0.9.8不兼容，所以不能通过创建软链接的方式解决，需要安装openssl0.9.8版本。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://mirrors.tuna.tsinghua.edu.cn/ubuntu/pool/universe/o/openssl098/libssl0.9.8_0.9.8o-7ubuntu3.2_i386.deb</span></span><br><span class="line">wget http://snapshot.debian.org/archive/debian/20110406T213352Z/pool/main/o/openssl098/libssl0.9.8_0.9.8o-7_i386.deb</span><br><span class="line">dpkg -i install libssl0.9.8_0.9.8o-7_i386.deb</span><br></pre></td></tr></table></figure><h2 id="Bug-TVision-error-Can-not-load-libcurses-so-Without-libcurses-can-work-only-with-xterm-linux"><a href="#Bug-TVision-error-Can-not-load-libcurses-so-Without-libcurses-can-work-only-with-xterm-linux" class="headerlink" title="Bug: TVision error: Can not load libcurses.so Without libcurses can work only with xterm/linux"></a>Bug: TVision error: Can not load libcurses.so Without libcurses can work only with xterm/linux</h2><p>设置环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TERM=xterm</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;IDA-Linux-Conf&quot;&gt;&lt;a href=&quot;#IDA-Linux-Conf&quot; class=&quot;headerlink&quot; title=&quot;IDA-Linux-Conf&quot;&gt;&lt;/a&gt;IDA-Linux-Conf&lt;/h1&gt;&lt;p&gt;Configure IDA pro linu</summary>
      
    
    
    
    
    <category term="Security" scheme="https://learner0x5a.github.io/tags/Security/"/>
    
    <category term="Tools" scheme="https://learner0x5a.github.io/tags/Tools/"/>
    
  </entry>
  
  <entry>
    <title>监视资本主义:智能陷阱</title>
    <link href="https://learner0x5a.github.io/2021/01/11/Social-Dilemma/"/>
    <id>https://learner0x5a.github.io/2021/01/11/Social-Dilemma/</id>
    <published>2021-01-11T07:05:17.000Z</published>
    <updated>2021-01-11T07:08:10.424Z</updated>
    
    <content type="html"><![CDATA[<p>网飞纪录片： Social Dilemma</p><h1 id="社交媒体的正反馈"><a href="#社交媒体的正反馈" class="headerlink" title="社交媒体的正反馈"></a>社交媒体的正反馈</h1><p>每次上/下拉刷新都是新东西。你不知道什么时候能刷到自己喜欢的东西，也不知道自己能刷到什么，就像老虎机一样。<br>如今注意力经济中的这种劝服性设计在人脑中植入无意识的习惯，对人进行重编程。你的判断力，对自我价值和身份的认知都在被重新编程。<br>在意别人的看法是必要的，但我们需要在意互联网上的10000个人的看法吗？我们真的需要每隔五分钟获得一次社交认可吗？<br>我们通过获得的满足感来管理自己的生活，每次点赞、爱心、大拇指这些信号给我们短期积极的反馈。所以社交媒体将这些和价值融合，和真相融合，后果是显而易见的。<br>而个人在一次次短期反馈之后，就会想“我接下来要做什么，我还想要这种感觉”，这就是恶性循环。</p><p>设计者会想方设法占用人的时间，比如消息提醒不交代清楚什么信息，“对方正在输入…”让你保持等待。而随着AI的发展，这些设计已经不是人类在做，而是算法在做。</p><p>社交媒体不是纯粹的工具，社交媒体有其目的：占用你的时间，以从广告商、政客等处收费盈利，而其采用的手段则是利用你的心理对付你自己。</p><p>楚门为什么没有发现所在世界的真实本质？因为我们接受了呈现在我们面前的世界就是现实。</p><p>我们创造了一个虚假消息的体系，虚假信息比真实信息更能让公司盈利，因为真相是无聊的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;网飞纪录片： Social Dilemma&lt;/p&gt;
&lt;h1 id=&quot;社交媒体的正反馈&quot;&gt;&lt;a href=&quot;#社交媒体的正反馈&quot; class=&quot;headerlink&quot; title=&quot;社交媒体的正反馈&quot;&gt;&lt;/a&gt;社交媒体的正反馈&lt;/h1&gt;&lt;p&gt;每次上/下拉刷新都是新东西。你不知</summary>
      
    
    
    
    
    <category term="others" scheme="https://learner0x5a.github.io/tags/others/"/>
    
  </entry>
  
  <entry>
    <title>西方政治思想史读书笔记</title>
    <link href="https://learner0x5a.github.io/2021/01/11/HistoryOfWesternPoliticalThoughts/"/>
    <id>https://learner0x5a.github.io/2021/01/11/HistoryOfWesternPoliticalThoughts/</id>
    <published>2021-01-10T16:39:41.000Z</published>
    <updated>2021-02-02T07:51:06.678Z</updated>
    
    <content type="html"><![CDATA[<h1 id="政治：众人之事"><a href="#政治：众人之事" class="headerlink" title="政治：众人之事"></a>政治：众人之事</h1><p>人与政治不可能完全隔绝，公共政策会影响日常生活，政治体制改革会影响一代人的命运。</p><p>以共和的视角来看，一个合格的公民应随时准备为政治世界的建设做贡献，由旁观者/思考者变为参与者。政治素养在实践中造就。</p><h1 id="古希腊：政治之始"><a href="#古希腊：政治之始" class="headerlink" title="古希腊：政治之始"></a>古希腊：政治之始</h1><h2 id="斯巴达"><a href="#斯巴达" class="headerlink" title="斯巴达"></a>斯巴达</h2><p>立法家莱库古改革：</p><pre><code>1. 设立元老院2. 均分土地与奴隶3. 取消商品交换，采用供给制4. 优生优育5. ......</code></pre><p><strong>想到了什么？</strong></p><h2 id="雅典"><a href="#雅典" class="headerlink" title="雅典"></a>雅典</h2><p>王政 -&gt; 贵族政治 -&gt; 僭主政治 -&gt; 民主政治 </p><p>僭主：武力夺取权力，没有经过合法程序。</p><h3 id="伯里克利演说"><a href="#伯里克利演说" class="headerlink" title="伯里克利演说"></a>伯里克利演说</h3><p><strong><em>我们爱好美丽的东西，但是不至于奢侈；我们爱好智慧，但不至于优柔。</em></strong></p><p><strong><em>财富是可以利用的东西，不是用来夸耀的东西。</em></strong></p><p><strong><em>他人的勇敢是因为无知，真正勇敢的人，是那个了解人生的幸福和灾难，然后勇往直前，担当起将来会发生的事故的人。</em></strong></p><p>在追求美的道路上，也会遇到最肮脏的恶。我预见了所有悲伤，但我欣然前往。</p><p><strong><em>他们贡献了他们的生命给城邦和我们所有人，至于他们自己，他们获得了永远长青的赞美和光辉灿烂的坟墓。</em></strong></p><p>《夜航星》的歌词也写得很好，“我是星，我愿投身前途未卜的群星，为梦长明，让希望做我无声永存的墓志铭。”</p><h3 id="苏格拉底"><a href="#苏格拉底" class="headerlink" title="苏格拉底"></a>苏格拉底</h3><p>以提问的方式鼓励他人思考，“我唯一知道的，是自己的无知。”</p><p>美德即知识。只有知道了什么是善，才能避免作恶。</p><h3 id="柏拉图"><a href="#柏拉图" class="headerlink" title="柏拉图"></a>柏拉图</h3><p>柏拉图要探求政治的理性基础。理想的城邦体现了绝对的善，是按理性的原则组织起来的，其基础是社会分工。<br>最好的城邦，不是抹平人与人之间的差异，而是在差异的基础上实现最优分工。</p><p>理想的国家以神话和宗教为基础。知识不能及之处，由宗教来补充。<br>神话和宗教是“高贵的谎言”，哲学家必须说谎以维持统治。</p><p>常人为“意见”所俘获，一生如在梦中，唯有哲学家是真理的代言人。<br>“意见”纷纭，“真理”只有一个。此即所谓“哲人王”。</p><p>“哲人王”是反民主的：意见之争，广场辩论应该让位于哲学家的真理之治。<br>但民主社会的问题就是，无人愿意承认谁是权威。</p><p>将政治问题转为教育问题，认为我们面临的公共危机根源在于个体的素质低下，是错误的。<br>政权更替，王朝兴衰并不意味着人民素质的突飞猛进。<br>诉诸人的培养以应对现实世界的问题，是教育救国的乌托邦，而教育也会沦为支配工具。</p><h3 id="不以物喜，不以己悲？"><a href="#不以物喜，不以己悲？" class="headerlink" title="不以物喜，不以己悲？"></a>不以物喜，不以己悲？</h3><p>人必然会受人生经历的局限，即便思想是自由的，也仍然会有浓重的时代烙印。<br>我们提倡独立思考，这主要是针对人云亦云/盲从权威的说法。<br>我们穷尽一生，都只是在以不同的方式回应时代的问题。</p><h3 id="亚里士多德"><a href="#亚里士多德" class="headerlink" title="亚里士多德"></a>亚里士多德</h3><p><strong>“人是天生的政治动物”</strong></p><p>城邦是一种为了实现共同的善而形成的结合体，城邦不仅是为了满足基本生活的需要，还要进一步追求一种美好的，高贵的生活。<br>政治关系是一种特定的平等自由的合作关系，既不是柏拉图所说统治者与被统治者的关系，也不是家庭中主人与奴隶的关系。<br>城邦的本质是政治自由与专业技能的结合。</p><p>然而探究什么是好的城邦，比较政体的优劣，只是服从于如何实现人的完善这一根本性的任务。<br>亚里士多德将生活分为享乐的生活，政治的生活和沉思的生活。三种都有其合理性，但沉思的生活是最幸福的，是理性充分活跃的。<br>沉思的生活追求至真，至善，至美。<br>沉思让人感到快乐，这种快乐纯粹，持久且自足，无需他人帮助。<br>沉思是无功利的，除了所沉思的问题之外不产生任何东西，不像在实践中，人们总在寻求得到什么。</p><p>但是，沉思的生活要求人们从城邦生活中退出，意味着放弃公民责任，对城邦造成毁灭性打击。</p><h2 id="希腊化时期"><a href="#希腊化时期" class="headerlink" title="希腊化时期"></a>希腊化时期</h2><h3 id="犬儒学派"><a href="#犬儒学派" class="headerlink" title="犬儒学派"></a>犬儒学派</h3><p>犬儒学派追求顺从天性，返璞归真。追求精神上的自主，不在乎别人的议论，要求个人与习俗决裂，与世界对抗。</p><p>犬儒学派同时也最早提出了“世界公民”的概念。真正的城邦只有一个，那就是宇宙。</p><h3 id="斯多葛学派"><a href="#斯多葛学派" class="headerlink" title="斯多葛学派"></a>斯多葛学派</h3><p>斯多葛学派强调理性的运用。人要学会控制自己的感情和欲望，存天理灭人欲，远离爱恨情仇，做一个圣人。<br>圣人最富有，最自由，是王者，是不死的神。</p><p>自然法：法律根植于自然，法律是一种自然力，是聪明人的理性，是衡量正义与否的标准。</p><p>斯多葛学派的思想类似于焦虑时代的心灵鸡汤。<br>“我们不能有福，但可以有善。只要我们有善，就让我们装成对不幸不计较。这种学说在一个恶劣的世界里是有用的，但却不是真实的。” —— 罗素</p><h3 id="伊壁鸠鲁学派"><a href="#伊壁鸠鲁学派" class="headerlink" title="伊壁鸠鲁学派"></a>伊壁鸠鲁学派</h3><p>伊壁鸠鲁学派提出了社会契约论。社会并非自然演化而成，而是建立在防止互相伤害的契约的基础上。<br>法律是用来提醒和惩戒一些不谨慎的人的，有良好的判断力的人无需法律。<br>这在古希腊时代是无神论。</p><h3 id="怀疑主义"><a href="#怀疑主义" class="headerlink" title="怀疑主义"></a>怀疑主义</h3><p>怀疑主义否定了人类获取绝对知识的可能性。正反两方面的道理都正确。<br>亚里士多德，斯多葛学派，伊壁鸠鲁学派都认为自己找到的通向真理的道路，而怀疑论者不相信自然法，不相信普遍道德准则的存在，一切法律都是人为创造出来的。<br>怀疑主义者不信任何教义，只寻求自我内在的平静。他们可以在任何习俗和法律中“幸福生活”。</p><p>古希腊哲学终止于怀疑主义。个人追求幸福是徒劳的，因为我们不知道何为幸福， 如何求得幸福。<br>哲学对真理的追求，探询，并不能导向某种确定的幸福生活。</p><p><em>古希腊人试图追求理性，追寻本质，最终却走向了怀疑主义。</em></p><h1 id="古罗马"><a href="#古罗马" class="headerlink" title="古罗马"></a>古罗马</h1><h2 id="共和"><a href="#共和" class="headerlink" title="共和"></a>共和</h2><p>罗马为何强大？<br>Πολυβιθσ(Polybius)说，纯粹的君主制/贵族制/民主制都容易腐化，隐含了权力使人腐化，绝对权力使人绝对腐化的观点。<br>最初君主凭其才德创建政体，但之后沦为僭主，贵族便联手将其推翻，确立贵族政体。<br>但是贵族们经过几代人之后，其不肖子孙骄奢淫逸，沦为寡头，激起人民的反抗。<br>人民建立民主政体，经过几代，自由平等不再受重视，人民挥霍享乐，任由野心家煽动，于是强人出现，又恢复了君主制。</p><p>单一的政体如此往复交替，而罗马人建立了由执政官/元老院/平民大会三种要素组成的政体，三者相互制约平衡。<br>罗马的混合政体影响深远，现代英美的政体都是典型的混合政体。<br>英国：国王，上议院，下议院；美国：总统，参议院，众议院。<br>一如罗马，分别对应君主/贵族/人民。</p><h2 id="帝国"><a href="#帝国" class="headerlink" title="帝国"></a>帝国</h2><p>共和晚期，统治精英素质衰退，元老不再是资深政治家，而是因为父辈进入元老院。<br>随着条顿人的入侵，罗马从亦农亦兵转为建立职业化军队，军人从效忠共和转为效忠统帅。<br>经由庞培，凯撒，奥古斯都，罗马从共和转为帝制。</p><p>由于人民的腐化，美德的丧失，共和时代一去不复返了。<br>知识分子不再讨论共和，而是在帝制前提下讨论政治，讨论一个合格的君主要有什么美德。<br>这与中华帝国儒家知识分子好谈君王有道无道，圣明昏聩，而不思考政体本身十分相似。<br>严格来说，这便只有伦理学而没有政治学，对当权者的道德说教毫无用处。<br>如同儒家的圣王理性，这种说教唤起了受奴役的人民心中对圣王的想象，原本对君王的道德训诫，成了王权的道德外衣。<br>人民之所以要服从君主，是因为君主在理性和道德上是完美的。</p><p>为了证明帝制中的人民拥有自由，建立在斯多葛学派自然法之上的罗马自有一套说法。<br>我们感到不自由不幸福，是因为我们错误地定义了自由和幸福。<br>我们总是从身外之物看待一切，而关键其实是内心的自由与幸福。<br>所有的灾难都是对美德的训练，人不应该去思考改变政体，改变外在的境遇，而是要着眼于自我道德的修炼。<br>这种观点，有时是一种自欺欺人。</p><h2 id="青山遮不住，毕竟东流去。"><a href="#青山遮不住，毕竟东流去。" class="headerlink" title="青山遮不住，毕竟东流去。"></a>青山遮不住，毕竟东流去。</h2><p>理解古典西方文明，需要把握其三个特点：城市文明，沿海文明，奴隶文明。<br>第三点对于理解古典西方文明的衰落十分重要。奴隶是经济的基础，当罗马帝国放弃扩张的帝国政策，奴隶主只能允许奴隶组建家庭实现奴隶的再生产。<br>由奴隶贸易支撑的商品经济转为以物易物的自然经济。而西方人称之为“黑暗的中世纪”。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;政治：众人之事&quot;&gt;&lt;a href=&quot;#政治：众人之事&quot; class=&quot;headerlink&quot; title=&quot;政治：众人之事&quot;&gt;&lt;/a&gt;政治：众人之事&lt;/h1&gt;&lt;p&gt;人与政治不可能完全隔绝，公共政策会影响日常生活，政治体制改革会影响一代人的命运。&lt;/p&gt;
&lt;p&gt;以共</summary>
      
    
    
    
    
    <category term="BookReading" scheme="https://learner0x5a.github.io/tags/BookReading/"/>
    
  </entry>
  
  <entry>
    <title>Linux 手动引导</title>
    <link href="https://learner0x5a.github.io/2021/01/09/linuxBoot/"/>
    <id>https://learner0x5a.github.io/2021/01/09/linuxBoot/</id>
    <published>2021-01-09T12:56:37.000Z</published>
    <updated>2021-01-09T13:08:08.547Z</updated>
    
    <content type="html"><![CDATA[<p>服务器断电后重启，引导出错，需要手动引导进入系统。</p><h1 id="Linux启动流程"><a href="#Linux启动流程" class="headerlink" title="Linux启动流程"></a>Linux启动流程</h1><ol><li>BIOS程序</li><li>GRUB引导</li><li>内核引导</li><li>调用虚拟文件系统initrd<ul><li>加载驱动</li><li>加载真正的根文件系统</li><li>执行/sbin/init</li><li>系统初始化</li></ul></li><li>启动终端</li></ol><h1 id="GRUB手动引导内核"><a href="#GRUB手动引导内核" class="headerlink" title="GRUB手动引导内核"></a>GRUB手动引导内核</h1><p>在grub shell中ls查看硬盘分区情况，常见的分区方案是，对于硬盘sdx，sdx1为boot分区(/boot)，sdx2为swap分区([SWAP])，sdx3为根分区(/)。</p><p>逐个尝试ls (hdx, y)/boot/grub，找到根文件系统所在分区（通常y=gpt3），也可以ls (hdx, y)查看该分区下所有文件。</p><p>如果手上有LiveCD，可以进LiveCD看一下分区情况。</p><p>所以该机器的引导参数如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定根文件系统</span></span><br><span class="line">$ root=(hd3,gpt3)</span><br><span class="line"><span class="comment"># 指定内核</span></span><br><span class="line">$ linux /boot/vmlinuz[Tab补全] root=/dev/sdb3</span><br><span class="line"><span class="comment"># 指定虚拟文件系统</span></span><br><span class="line">$ initrd /boot/initrd[Tab补全].img</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">$ boot</span><br></pre></td></tr></table></figure><p>tab补全时，虽然有四个内核可选，但有两个加载失败，最后加载5.3.0-62及对应的initrd，引导成功。</p><h1 id="从U盘进入Livecd，查看文件系统-磁盘状态"><a href="#从U盘进入Livecd，查看文件系统-磁盘状态" class="headerlink" title="从U盘进入Livecd，查看文件系统/磁盘状态"></a>从U盘进入Livecd，查看文件系统/磁盘状态</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df -h <span class="comment"># 查看已挂载的磁盘分区及挂载点</span></span><br><span class="line">fdisk -l <span class="comment"># 查看磁盘分区，包括未挂载的磁盘</span></span><br><span class="line">lsblk -l <span class="comment"># 查看磁盘分区，包括未挂载的磁盘</span></span><br></pre></td></tr></table></figure><p>可以发现，当前boot在sdd，挂载点是/cdrom，即sdd是U盘上livecd的系统。</p><p>查看另外的磁盘，找到EFI分区和swap分区所在的磁盘，比如sdb，说明原来的系统在sdb上。</p><p>根文件系统在Type:Linux所在的分区，例如/dev/sdb3，对应到grub里面，应该是某块硬盘的第三个分区，例如(hd3,gpt3)。</p><p><strong>注意：USB的插入会影响grub中硬盘的编号，同一块硬盘有时候是hd3，有时候是hd4</strong></p><h1 id="如何判断文件系统是否需要修复？"><a href="#如何判断文件系统是否需要修复？" class="headerlink" title="如何判断文件系统是否需要修复？"></a>如何判断文件系统是否需要修复？</h1><p>一般情况下，linux系统启动阶段，如果文件系统出现了crash，会提示有orphan node，无法正常启动。此时，需要使用fsck来修复文件系统。一般来说，异常断电就可能出现这种情况。</p><p>异常断电情况下，需要手动按照之前记录的挂载目录参数来手动进行引导。如果手动引导仍然无法成功，大概率是文件系统挂了，此时需要先修复文件系统，再进入grub中手动引导。</p><p>grub启动引导记录的修复（通过boot-repair修复后，再手动检查grub启动引导配置文件是否无误），可以保证正常重启服务器是没问题的。</p><p>但是通常异常断电的情况下，grub引导又会出错从而导致系统无法正常被引导启动。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;服务器断电后重启，引导出错，需要手动引导进入系统。&lt;/p&gt;
&lt;h1 id=&quot;Linux启动流程&quot;&gt;&lt;a href=&quot;#Linux启动流程&quot; class=&quot;headerlink&quot; title=&quot;Linux启动流程&quot;&gt;&lt;/a&gt;Linux启动流程&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;BIO</summary>
      
    
    
    
    
    <category term="others" scheme="https://learner0x5a.github.io/tags/others/"/>
    
  </entry>
  
  <entry>
    <title>Win10激活-自建KMS服务器</title>
    <link href="https://learner0x5a.github.io/2021/01/08/kms/"/>
    <id>https://learner0x5a.github.io/2021/01/08/kms/</id>
    <published>2021-01-08T07:59:30.000Z</published>
    <updated>2021-01-08T08:18:44.377Z</updated>
    
    <content type="html"><![CDATA[<h1 id="KMS-server-docker"><a href="#KMS-server-docker" class="headerlink" title="KMS server - docker"></a>KMS server - docker</h1><p><a href="https://github.com/Wind4/vlmcsd-docker">vlmcsd</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/Wind4/vlmcsd-docker.git vlmcsd</span><br><span class="line"><span class="built_in">cd</span> vlmcsd</span><br><span class="line"><span class="comment"># Use docker-compose service</span></span><br><span class="line">docker-compose up -d</span><br><span class="line"><span class="comment"># or docker build image</span></span><br><span class="line"><span class="comment"># docker build -t vlmcsd .</span></span><br><span class="line"><span class="comment"># docker run -idt -p 1688:1688 vlmcsd</span></span><br></pre></td></tr></table></figure><h1 id="KMS-client-Windows-10"><a href="#KMS-client-Windows-10" class="headerlink" title="KMS client - Windows 10"></a>KMS client - Windows 10</h1><ol><li><p>管理员权限运行cmd，卸载之前的key：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr.vbs -upk</span><br></pre></td></tr></table></figure></li><li><p>安装kms客户端key（去<a href="https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/jj612867(v=ws.11)">官网</a>找自己版本的key，这里是教育版的key）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr.vbs -ipk NW6C2-QMPVW-D7KKK-3GKT6-VCFB2</span><br></pre></td></tr></table></figure></li><li><p>设置kms服务器，ip:port换成上面kms server docker的ip:port，默认端口是1688</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr.vbs -skms 1.2.3.4:6666</span><br></pre></td></tr></table></figure></li><li><p>连线激活(稍等一会)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr.vbs -ato</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;KMS-server-docker&quot;&gt;&lt;a href=&quot;#KMS-server-docker&quot; class=&quot;headerlink&quot; title=&quot;KMS server - docker&quot;&gt;&lt;/a&gt;KMS server - docker&lt;/h1&gt;&lt;p&gt;&lt;a hre</summary>
      
    
    
    
    
    <category term="others" scheme="https://learner0x5a.github.io/tags/others/"/>
    
  </entry>
  
  <entry>
    <title>本地文件丢失，如何恢复hexo博客</title>
    <link href="https://learner0x5a.github.io/2021/01/06/restore/"/>
    <id>https://learner0x5a.github.io/2021/01/06/restore/</id>
    <published>2021-01-06T12:45:16.000Z</published>
    <updated>2021-01-06T12:45:49.240Z</updated>
    
    <content type="html"><![CDATA[<h2 id="恢复hexo"><a href="#恢复hexo" class="headerlink" title="恢复hexo"></a>恢复hexo</h2><h3 id="重新init"><a href="#重新init" class="headerlink" title="重新init"></a>重新init</h3><p>hexo之前push到git上的只有生成的html/图片，没有md文件。</p><p>如何恢复？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install html2text</span><br></pre></td></tr></table></figure><p>把html转为文本，再手动重建md。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;恢复hexo&quot;&gt;&lt;a href=&quot;#恢复hexo&quot; class=&quot;headerlink&quot; title=&quot;恢复hexo&quot;&gt;&lt;/a&gt;恢复hexo&lt;/h2&gt;&lt;h3 id=&quot;重新init&quot;&gt;&lt;a href=&quot;#重新init&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    
    <category term="others" scheme="https://learner0x5a.github.io/tags/others/"/>
    
  </entry>
  
  <entry>
    <title>树莓派4b</title>
    <link href="https://learner0x5a.github.io/2020/12/24/rpi/"/>
    <id>https://learner0x5a.github.io/2020/12/24/rpi/</id>
    <published>2020-12-24T12:16:39.000Z</published>
    <updated>2021-01-06T09:15:42.276Z</updated>
    
    <content type="html"><![CDATA[<h1 id="树莓派4b折腾笔记"><a href="#树莓派4b折腾笔记" class="headerlink" title="树莓派4b折腾笔记"></a>树莓派4b折腾笔记</h1><h2 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h2><p>如果OS运行在TF卡上，一定要用高速TF卡，我选用的是SanDisk Extreme PRO 32 GB。</p><h2 id="装系统"><a href="#装系统" class="headerlink" title="装系统"></a><a href="https://ubuntu.com/tutorials/how-to-install-ubuntu-desktop-on-raspberry-pi-4#2-prepare-the-sd-card">装系统</a></h2><p>装完系统先给LAN<a href="https://www.cnblogs.com/blueyunchao0618/p/11394640.html">配个静态ip</a></p><p>考虑到图形界面的开销很大，我装了一个Ubuntu server 20.04，TF卡镜像写完开机即用，无需安装系统。</p><p>同时由于没有图形界面，需要连Wifi的话：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ip link <span class="comment"># 查看各个网卡状态</span></span><br><span class="line">sudo iw dev wlan0 scan | grep SSID <span class="comment"># 扫描附近的SSID</span></span><br><span class="line">wpa_passphrase <span class="string">&quot;&lt;SSID&gt;&quot;</span> <span class="string">&quot;&lt;Password&gt;&quot;</span> <span class="comment"># 输出配置模板到stdout</span></span><br><span class="line">sudo vim /etc/wpa_supplicant/wpa_supplicant.conf <span class="comment"># 按上述模板配置WiFi</span></span><br><span class="line">sudo wpa_supplicant -B -D nl80211 -i wlan0 -c /etc/wpa_supplicant/wpa_supplicant.conf <span class="comment"># 连接Wi-Fi</span></span><br></pre></td></tr></table></figure><h2 id="软件源"><a href="#软件源" class="headerlink" title="软件源"></a>软件源</h2><p><code>uname -a</code>看一眼架构为<code>aarch64</code>属于arm64架构，选用<a href="https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu-ports/">Ubuntu-ports源</a></p><h2 id="尝试作为服务器"><a href="#尝试作为服务器" class="headerlink" title="尝试作为服务器"></a>尝试作为服务器</h2><p>连接到华为随行wifi3之后，即使配了端口转发也无法连接。</p><p>查看wifi的wan口ip发现是个10.x.x.x的局域网ip，为什么？也许就是因为这个？</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;树莓派4b折腾笔记&quot;&gt;&lt;a href=&quot;#树莓派4b折腾笔记&quot; class=&quot;headerlink&quot; title=&quot;树莓派4b折腾笔记&quot;&gt;&lt;/a&gt;树莓派4b折腾笔记&lt;/h1&gt;&lt;h2 id=&quot;硬件&quot;&gt;&lt;a href=&quot;#硬件&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    
    <category term="OS" scheme="https://learner0x5a.github.io/tags/OS/"/>
    
  </entry>
  
  <entry>
    <title>基于源代码的机器学习</title>
    <link href="https://learner0x5a.github.io/2020/12/14/MLonSrcCode/"/>
    <id>https://learner0x5a.github.io/2020/12/14/MLonSrcCode/</id>
    <published>2020-12-14T11:57:03.000Z</published>
    <updated>2020-12-14T11:58:19.094Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基于源代码的机器学习"><a href="#基于源代码的机器学习" class="headerlink" title="基于源代码的机器学习"></a>基于源代码的机器学习</h1><h2 id="ML4CODE"><a href="#ML4CODE" class="headerlink" title="ML4CODE"></a><a href="https://ml4code.github.io/">ML4CODE</a></h2><p>本领域主要关注三方面问题：</p><pre><code>  * 代码生成  * 代码表示  * 模式识别</code></pre><h2 id="CodRep"><a href="#CodRep" class="headerlink" title="CodRep"></a><a href="https://github.com/KTH/CodRep-competition">CodRep</a></h2><p>代码表示学习的比赛，给定源代码，要求修补源代码中的bug</p><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><h3 id="程序生成"><a href="#程序生成" class="headerlink" title="程序生成"></a>程序生成</h3><p>Synthetic Datasets for Neural Program Synthesis, ICLR 2019<br>Execution-Guided Neural Program Synthesis, ICLR 2019<br>DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing,<br>AAAI 2019<br>Towards Synthesizing Complex Programs from Input-Output Examples, ICLR 2018</p><h3 id="源代码分析及语言模型"><a href="#源代码分析及语言模型" class="headerlink" title="源代码分析及语言模型"></a>源代码分析及语言模型</h3><p>Generative Code Modeling with Graphs, ICLR 2019<br>NL2Type: Inferring JavaScript Function Types from Natural Language<br>Information, ICSE 2019<br>A Novel Neural Source Code Representation based on Abstract Syntax Tree, ICSE<br>2019<br>Deep Learning Type Inference, FSE 2018<br>Learning to Represent Programs with Graphs, ICLR 2018<br>Are Deep Neural Networks the Best Choice for Modeling Source Code?, FSE 2017</p><h3 id="神经网络结构与算法"><a href="#神经网络结构与算法" class="headerlink" title="神经网络结构与算法"></a>神经网络结构与算法</h3><p>Neural Code Comprehension: A Learnable Representation of Code Semantics,<br>NeurIPS 2018<br>A General Path-Based Representation for Predicting Program Properties, PLDI<br>2018<br>Cross-Language Learning for Program Classification using Bilateral Tree-Based<br>Convolutional Neural Networks, AAAI 2018</p><h3 id="嵌入"><a href="#嵌入" class="headerlink" title="嵌入"></a>嵌入</h3><p>A Literature Study of Embeddings on Source Code, arxiv<br>Deep Code Search, ICSE 2018<br>Code Vectors: Understanding Programs Through Embedded Abstracted Symbolic<br>Traces, FSE 2018</p><h3 id="程序翻译"><a href="#程序翻译" class="headerlink" title="程序翻译"></a>程序翻译</h3><p>Towards Neural Decompilation, arxiv<br>Tree-to-tree Neural Networks for Program Translation, ICLR 2018<br>Code Attention: Translating Code to Comments by Exploiting Domain Features,<br>arxiv</p><h3 id="代码补全"><a href="#代码补全" class="headerlink" title="代码补全"></a>代码补全</h3><p>Aroma: Code Recommendation via Structural Code Search, arxiv</p><h3 id="程序修复"><a href="#程序修复" class="headerlink" title="程序修复"></a>程序修复</h3><p>Maximal Divergence Sequential Autoencoder for Binary Software Vulnerability<br>Detection, ICLR 2019<br>Neural Program Repair by Jointly Learning to Localize and Repair, ICLR 2019<br>Dynamic Neural Program Embedding for Program Repair, ICLR 2018</p><h3 id="模式识别"><a href="#模式识别" class="headerlink" title="模式识别"></a>模式识别</h3><p>SAR: Learning Cross-Language API Mappings with Little Knowledge, FSE 2019<br>Hierarchical Learning of Cross-Language Mappings through Distributed Vector<br>Representations for Code, ICSE 2018<br>Deep API Learning, FSE 2016</p><h3 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h3><p>Neural Nets Can Learn Function Type Signatures From Binaries, USENIX 2017</p><h3 id="代码总结"><a href="#代码总结" class="headerlink" title="代码总结"></a>代码总结</h3><p>Summarizing Source Code with Transferred API Knowledge, IJCAI 2018<br>A Neural Framework for Retrieval and Summarization of Source Code, ASE 2018</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;基于源代码的机器学习&quot;&gt;&lt;a href=&quot;#基于源代码的机器学习&quot; class=&quot;headerlink&quot; title=&quot;基于源代码的机器学习&quot;&gt;&lt;/a&gt;基于源代码的机器学习&lt;/h1&gt;&lt;h2 id=&quot;ML4CODE&quot;&gt;&lt;a href=&quot;#ML4CODE&quot; class</summary>
      
    
    
    
    
    <category term="AI" scheme="https://learner0x5a.github.io/tags/AI/"/>
    
    <category term="Security" scheme="https://learner0x5a.github.io/tags/Security/"/>
    
  </entry>
  
  <entry>
    <title>attention机制的理解</title>
    <link href="https://learner0x5a.github.io/2020/12/14/attention/"/>
    <id>https://learner0x5a.github.io/2020/12/14/attention/</id>
    <published>2020-12-14T11:51:22.000Z</published>
    <updated>2021-02-18T13:31:57.808Z</updated>
    
    <content type="html"><![CDATA[<h1 id="seq2seq-amp-attention"><a href="#seq2seq-amp-attention" class="headerlink" title="seq2seq &amp; attention"></a>seq2seq &amp; attention</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>大多数attention都是时域attention，即关注输入序列（或者encoder的输出序列）中各个timestep的数据的不同贡献<br>那么频域attention该怎么做呢？比如要关注图像的某一部分对分类器的贡献？<br>最普通的，例如一个<code>MLP1 + Attention + MLP2</code>的结构，MLP的输出shape为(batch_size,hidden_dim)，即timestep为1，怎么算attention？<br>timestep为1的时候，softmax-&gt;sigmoid，概率分布不存在了，按原算法得到的attention是一个shape为(batch_size,1,1)的张量乘上encoder_output，相当于对每个feature乘上一个实数，意义是什么？</p><p>Update 2021-02-18: 参考<a href="https://blog.csdn.net/u010848594/article/details/107413620">这里</a>。<br>把握住attention的本质是加权，由此，<br>卷积网络里有两种attention，一种是通道attention，对各个feature map加权；一种是空间attention，利用卷积运算获得spatial attention。</p><h1 id="PyTorch-Misc"><a href="#PyTorch-Misc" class="headerlink" title="PyTorch Misc"></a>PyTorch Misc</h1><h2 id="LSTM的返回值"><a href="#LSTM的返回值" class="headerlink" title="LSTM的返回值"></a>LSTM的返回值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output(seq_len, batch, hidden_size * num_directions) <span class="comment"># 每个timestep，最后(深)一层的神经元输出的拼接  </span></span><br><span class="line">hn(num_layers * num_directions, batch, hidden_size) <span class="comment"># 最后一个timestep，所有神经元的输出  </span></span><br><span class="line">cn(num_layers * num_directions, batch, hidden_size) <span class="comment"># 最后一个timestep，所有神经元的细胞状态  </span></span><br></pre></td></tr></table></figure><h2 id="inplace-True"><a href="#inplace-True" class="headerlink" title="inplace=True"></a>inplace=True</h2><p>激活函数例如<code>nn.LeakyReLU(inplace=True)</code>中<code>inplace=True</code>的意思是进行原地操作，例如<code>x=x+5</code>，对于Conv2d这样的上层网络传递下来的tensor直接进行修改，好处就是可以节省运算内存。</p><h2 id="dim-1"><a href="#dim-1" class="headerlink" title="dim=-1"></a>dim=-1</h2><p><code>dim=-1</code>意为对最后一维进行操作</p><h2 id="permute-dims"><a href="#permute-dims" class="headerlink" title="permute(dims)"></a>permute(dims)</h2><p>将tensor的维度换位</p><p>permute(0,2,1) -&gt; 将后两维对换</p><h2 id="torch-cat-tensors-dim"><a href="#torch-cat-tensors-dim" class="headerlink" title="torch.cat(tensors, dim)"></a>torch.cat(tensors, dim)</h2><p>按dim拼接tensors</p><h2 id="torch-chunk-tensor-chunk-num-dim"><a href="#torch-chunk-tensor-chunk-num-dim" class="headerlink" title="torch.chunk(tensor, chunk_num, dim)"></a>torch.chunk(tensor, chunk_num, dim)</h2><p>与torch.cat()原理相反，它是将tensor按dim（行或列）分割成chunk_num个tensor块，返回的是一个元组。</p><h2 id="LSTM原理"><a href="#LSTM原理" class="headerlink" title="LSTM原理"></a>LSTM原理</h2><p><img src="/images/LSTM.png" alt="LSTM"></p><h2 id="Attention原理"><a href="#Attention原理" class="headerlink" title="Attention原理"></a>Attention<a href="https://www.zhihu.com/question/68482809/answer/264632289">原理</a></h2><p><img src="/images/attention1.jpg" alt="Attention"><br><img src="/images/attention2.jpg" alt="Attention"><br><img src="/images/attention3.jpg" alt="Attention"></p><h2 id="Attention详解"><a href="#Attention详解" class="headerlink" title="Attention详解"></a>Attention详解</h2><p>seq2seq pipeline: input vector -&gt; encoder -&gt; context vector -&gt; decoder -&gt; output vector</p><p>一般的attention<a href="https://zhuanlan.zhihu.com/p/62486641">公式</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">M = tanh(lstm_output)  </span><br><span class="line">alpha = softmax(att_weights.dot(M))  </span><br><span class="line">context_vector = lstm_output.dot(alpha.T)  </span><br></pre></td></tr></table></figure><p>att_weight 是一个(1, hidden_dim)的向量<br>对应attention本质模型，key=value=lstm_ouput，query向量在哪呢？ – 在att_weights中<br>query是学习出来的，不然attention就成了先验知识了<br>在<a href="https://blog.csdn.net/dendi_hust/article/details/94435919">这里</a>看到：把att_weight用MLP(lstm_hidden_state)代替（因为decoder的隐藏层输出和encoder的隐藏层输出处在同一个子空间内），更有道理一些，不知道效果怎么样<br>要计算query和key的相似度，可以通过內积的方式表征，更一般的，可以写作F(query,key)，怎么定义随便，F甚至可以是NN；进一步推广，将query嵌入NN中，直接NN(lstm_output)即为相似度<br>算完相似度之后，用softmax计算归一化的概率分布，再乘上value即可。<br>自注意力机制即K=V=Q，即为query=G(value) &lt;==&gt; att_weights = G(lstm_outputs), G为任意变换，比如NN</p><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>Attention is all you need<br>提出<a href="https://zhuanlan.zhihu.com/p/44121378">Transformer</a>架构<br>encoder部分，输入数据经过Embedding之后，直接接多头自注意力层，不再先经过特征提取层例如CNN或者LSTM，然后再接一个MLP变换 +<br>残差，送入decoder</p><p>Transformer为每个输入单词的词嵌入上添加了一个新向量——位置向量，即embedding_with_time_signal =<br>positional_encoding + embedding</p><p><img src="/images/Transformer.jpg" alt="Transformer"></p><h3 id="多头attention（Multi-head-attention）"><a href="#多头attention（Multi-head-attention）" class="headerlink" title="多头attention（Multi-head attention）"></a>多头attention（Multi-head attention）</h3><p><img src="/images/Attention_Mask&Multi-head.jpg" alt="Mask&amp;Multi-head"></p><p>结构如图，Query，Key，Value首先进过一个线性变换，然后输入到放缩点积attention<br>注意这里要做h次，也就是所谓的多头，每一次算一个头，头之间参数不共享，每次Q，K，V进行线性变换的参数是不一样的。<br>然后将h次的放缩点积attention结果进行拼接，再进行一次线性变换得到的值作为多头attention的结果。<br>论文中说到这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息。</p><p>代码实现上，即将Query/Key/Value都reshape一下<br>(batch_sze, timesteps, hidden_dims) -&gt; (batch_sze*num_heads, timesteps, hidden_dims/num_heads)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span>                    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model_dim=<span class="number">512</span>, num_heads=<span class="number">8</span>, dropout=<span class="number">0.0</span></span>):</span>  </span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()  </span><br><span class="line">    </span><br><span class="line">        self.dim_per_head = model_dim // num_heads <span class="comment"># &quot; / &quot;表示 浮点数除法，返回浮点结果;&quot; // &quot;表示整数除法  </span></span><br><span class="line">        self.num_heads = num_heads  </span><br><span class="line">        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)  </span><br><span class="line">        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)  </span><br><span class="line">        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)  </span><br><span class="line">    </span><br><span class="line">        self.dot_product_attention = ScaledDotProductAttention(dropout)  </span><br><span class="line">        self.linear_final = nn.Linear(model_dim, model_dim)  </span><br><span class="line">        self.dropout = nn.Dropout(dropout)  </span><br><span class="line">        <span class="comment"># multi-head attention之后需要做layer norm  </span></span><br><span class="line">        self.layer_norm = nn.LayerNorm(model_dim)  </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, key, value, query, attn_mask=<span class="literal">None</span></span>):</span>  </span><br><span class="line">        <span class="comment"># 残差连接  </span></span><br><span class="line">        residual = query  </span><br><span class="line">    </span><br><span class="line">        dim_per_head = self.dim_per_head  </span><br><span class="line">        num_heads = self.num_heads  </span><br><span class="line">        batch_size = key.size(<span class="number">0</span>)  </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># linear projection  </span></span><br><span class="line">        key = self.linear_k(key)  </span><br><span class="line">        value = self.linear_v(value)  </span><br><span class="line">        query = self.linear_q(query)  </span><br><span class="line">                      </span><br><span class="line">        <span class="comment"># split by heads  </span></span><br><span class="line">        key = key.view(batch_size * num_heads, -<span class="number">1</span>, dim_per_head)  </span><br><span class="line">        value = value.view(batch_size * num_heads, -<span class="number">1</span>, dim_per_head)  </span><br><span class="line">        query = query.view(batch_size * num_heads, -<span class="number">1</span>, dim_per_head)  </span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> attn_mask:  </span><br><span class="line">            attn_mask = attn_mask.repeat(num_heads, <span class="number">1</span>, <span class="number">1</span>)  </span><br><span class="line">        <span class="comment"># scaled dot product attention  </span></span><br><span class="line">        scale = (key.size(-<span class="number">1</span>) // num_heads) ** -<span class="number">0.5</span>  </span><br><span class="line">        context, attention = self.dot_product_attention(  </span><br><span class="line">            query, key, value, scale, attn_mask)  </span><br><span class="line">        <span class="comment"># concat heads  </span></span><br><span class="line">        context = context.view(batch_size, -<span class="number">1</span>, dim_per_head * num_heads)  </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># final linear projection  </span></span><br><span class="line">        output = self.linear_final(context)  </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># dropout  </span></span><br><span class="line">        output = self.dropout(output)  </span><br><span class="line">    </span><br><span class="line">        <span class="comment"># add residual and norm layer  </span></span><br><span class="line">        output = self.layer_norm(residual + output)  </span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> output, attention  </span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a href="https://www.jianshu.com/p/e7d8caa13b21">参考</a><br><a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">Transformer Code</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;seq2seq-amp-attention&quot;&gt;&lt;a href=&quot;#seq2seq-amp-attention&quot; class=&quot;headerlink&quot; title=&quot;seq2seq &amp;amp; attention&quot;&gt;&lt;/a&gt;seq2seq &amp;amp; attenti</summary>
      
    
    
    
    
    <category term="AI" scheme="https://learner0x5a.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>基于酷Q开发的QQ聊天机器人</title>
    <link href="https://learner0x5a.github.io/2020/12/14/%E5%9F%BA%E4%BA%8E%E9%85%B7Q%E5%BC%80%E5%8F%91%E7%9A%84QQ%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    <id>https://learner0x5a.github.io/2020/12/14/%E5%9F%BA%E4%BA%8E%E9%85%B7Q%E5%BC%80%E5%8F%91%E7%9A%84QQ%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/</id>
    <published>2020-12-14T11:41:16.000Z</published>
    <updated>2020-12-14T11:47:44.998Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基于酷Q开发的QQ聊天机器人"><a href="#基于酷Q开发的QQ聊天机器人" class="headerlink" title="基于酷Q开发的QQ聊天机器人"></a>基于酷Q开发的QQ聊天机器人</h1><p>接入腾讯AI开放平台需要计算资源较好以及网络条件较好，不然延迟太大，不能支持高频率聊天或者高并发聊天。</p><h2 id="安装酷Q"><a href="#安装酷Q" class="headerlink" title="安装酷Q"></a>安装酷Q</h2><p><a href="https://cqp.cc/">酷Q</a>机器人是一款比较成熟的QQ机器人，建议开一个QQ小号玩~<br>安装完直接打开，登录小号，配置应用，可以先走一遍互动式教程，非常容易上手。<br>酷Q的图灵机器人需要另外申请，最可恨的是实名制之后每天也就100次的调用次数（即聊天回复累积多于100条时就GG），限制太死。</p><h2 id="插件开发：-C-SDK"><a href="#插件开发：-C-SDK" class="headerlink" title="插件开发： C++ SDK"></a>插件开发： C++ <a href="https://cqcppsdk.cqp.moe/guide/getting-started.html#%E5%87%86%E5%A4%87">SDK</a></h2><h3 id="C-SDK的配置"><a href="#C-SDK的配置" class="headerlink" title="C++ SDK的配置"></a>C++ SDK的配置</h3><p>首先下载/克隆项目，安装Visual Studio 2019<br>直接用VS 2019打开 awesome-bot 文件夹，VS 将会自动进行 CMake 配置，产生的 build 目录在 out<br>中，点击菜单栏的「生成」-「全部生成」即可构建，产生的 app.dll 和 app_dev.exe 文件在 out/build/config/ 中。</p><h3 id="编译插件"><a href="#编译插件" class="headerlink" title="编译插件"></a>编译插件</h3><p>用VS 2019的x86-Release模式生成即可</p><h3 id="集成到酷Q"><a href="#集成到酷Q" class="headerlink" title="集成到酷Q"></a>集成到酷Q</h3><p>打开酷Q的开发模式<br>然后将awesome-bot中的app.json和awesome-bot/build中的app.dll一同放进酷Q目录的dev/com.example.demo目录（需手动创建）中<br>再重启酷Q或重载应用即可在应用管理中看到demo应用，启用即可。</p><p>SDK自带了一个demo，私聊是一个复读机：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">on_private_message([](<span class="keyword">const</span> PrivateMessageEvent &amp;e) &#123;  </span><br><span class="line">    <span class="keyword">try</span> &#123;  </span><br><span class="line">        <span class="keyword">auto</span> msgid = send_private_message(e.user_id, e.message); <span class="comment">// 直接复读消息  </span></span><br><span class="line">        logging::info_success(<span class="string">&quot;私聊&quot;</span>, <span class="string">&quot;私聊消息复读完成, 消息 Id: &quot;</span> + to_string(msgid));  </span><br><span class="line">        send_message(e.target,  </span><br><span class="line">                        MessageSegment::face(<span class="number">111</span>) + <span class="string">&quot;这是通过 message 模块构造的消息~&quot;</span>); <span class="comment">// 使用 message 模块构造消息  </span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (ApiError &amp;e) &#123;  </span><br><span class="line">        logging::warning(<span class="string">&quot;私聊&quot;</span>, <span class="string">&quot;私聊消息复读失败, 错误码: &quot;</span> + to_string(e.code));  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;);  </span><br><span class="line">``` </span><br><span class="line">  </span><br><span class="line">## 接入腾讯AI开放平台</span><br><span class="line"></span><br><span class="line">复读机有点太蠢了，考虑用一下现成的AI  </span><br><span class="line">腾讯AI开放平台有一个[智能闲聊](https:<span class="comment">//ai.qq.com/product/nlpchat.shtml)的功能，考虑用一下  </span></span><br><span class="line">调用智能闲聊的API需要HTTP通信和json解析</span><br><span class="line"></span><br><span class="line">### [jsoncpp](https:<span class="comment">//github.com/open-source-parsers/jsoncpp)的安装</span></span><br><span class="line"></span><br><span class="line">最新版的安装方法如下（需要cmake）：</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">cd json-master  </span><br><span class="line">cmake.exe CMakeList.txt  </span><br><span class="line">```                    </span><br><span class="line">  </span><br><span class="line">---|---  </span><br><span class="line">  </span><br><span class="line">得到VS工程文件`ALL_BUILD.vcxproj`，打开编译即可。  </span><br><span class="line">但这里遇到一个问题，酷Q插件要求必须是x86的PE，但是最新版jsoncpp似乎不支持x86了。  </span><br><span class="line">下载jsoncpp <span class="number">0.5</span><span class="number">.0</span>版，打开解决方案jsoncpp-src<span class="number">-0.5</span><span class="number">.0</span>/makefiles/vs71/jsoncpp.sln（vs71即VS2003，高版本VS默认转换即可）  </span><br><span class="line">编译release版时，注意修改生成静态库文件的工程的属性：菜单 - 项目 - 属性 - 配置属性 - C/C++ - 输出文件 - 汇编程序输出 改为</span><br><span class="line">无列表</span><br><span class="line"></span><br><span class="line">##<span class="meta"># jsoncpp的使用</span></span><br><span class="line"></span><br><span class="line">VS中新建一个win32控制台工程，项目属性配置如下：</span><br><span class="line"></span><br><span class="line">    * C/C++ - 附加包含目录 设为jsoncpp源码的include目录，例如jsoncpp-src<span class="number">-0.5</span><span class="number">.0</span>/include/json，或者直接将这个文件夹复制到cpp文件目录下</span><br><span class="line">    * C/C++ - 代码生成 - 运行库和jsoncpp编译时保持一致 </span><br><span class="line">    * 链接器 - 常规 - 附加库目录 添加jsoncpp工程编译出的静态库所在的目录</span><br><span class="line">    * 链接器 - 输入 - 附加依赖项 添加jsoncpp工程编译出的静态库的文件名</span><br><span class="line"></span><br><span class="line">编写源代码时，引用方法如下：</span><br><span class="line"></span><br><span class="line">```C++   </span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fstream&gt;  </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;json/json.h&quot;</span>  </span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line">    </span><br><span class="line"><span class="keyword">int</span> main()  </span><br><span class="line">&#123;  </span><br><span class="line">    Json::Value value1;  </span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 给字段赋值，key必须为string型  </span></span><br><span class="line">    <span class="comment">// 类似STL的map，访问一个不存在的字段时会自动新建一个字段     </span></span><br><span class="line">    value1[<span class="string">&quot;key&quot;</span>] = <span class="string">&quot;value&quot;</span>;  </span><br><span class="line">```                      </span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">### HTTP通信</span><br><span class="line"></span><br><span class="line">C++的HTTP通信比较麻烦，可以借助libcurl实现 -&gt; Visual Studio</span><br><span class="line"><span class="number">2017</span>/<span class="number">2019</span>的libcurl[编译](https:<span class="comment">//stackoverflow.com/questions/53861300/how-do-you-properly-install-libcurl-for-use-in-visual-studio-2017/54680718#54680718)</span></span><br><span class="line"></span><br><span class="line">    * 这里碰到一个bug，jsoncpp和libcurl分别编译都能通过，但是放到同一个项目里面的时候不兼容，MT/MD的问题，后来干脆都不用</span><br><span class="line">    * 根据腾讯AI开放平台的[通信协议](https:<span class="comment">//ai.qq.com/doc/auth.shtml#3-%E6%9C%80%E7%BB%88%E8%AF%B7%E6%B1%82%E6%95%B0%E6%8D%AE)</span></span><br><span class="line">    * 计算完sign之后把参数字典做url编码(即实现一下PHP的`http_build_query()`函数)，写到文件`CQ_question`用Python处理通信，C++和Python用文件交互即可</span><br><span class="line">                    </span><br><span class="line"></span><br><span class="line">```python                    </span><br><span class="line">def get_content():  </span><br><span class="line">    url = <span class="string">&quot;https://api.ai.qq.com/fcgi-bin/nlp/nlp_textchat&quot;</span>      </span><br><span class="line">    f = open(<span class="string">&quot;CQ_question&quot;</span>,<span class="string">&quot;r&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>)  </span><br><span class="line">    question = f.readlines()[<span class="number">0</span>]  </span><br><span class="line">    f.close()  </span><br><span class="line">    r = requests.post(url,data=question)  </span><br><span class="line">    <span class="keyword">return</span> r.json()[<span class="string">&quot;data&quot;</span>][<span class="string">&quot;answer&quot;</span>]  </span><br><span class="line">    </span><br><span class="line">f1 = open(<span class="string">&quot;CQ_answer&quot;</span>,<span class="string">&quot;w&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>)  </span><br><span class="line">f1.write(answer)  </span><br><span class="line">f1.close()  </span><br></pre></td></tr></table></figure><p>计算sign的过程中用到了<a href="https://blog.csdn.net/WeDeserveIt/article/details/81950776">md5</a>和<a href="https://github.com/int2e/UrlEncoder">url编码</a><br>C++的字典用map实现</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">    <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="built_in">string</span>&gt; paras;  </span><br><span class="line">    paras[<span class="string">&quot;app_id&quot;</span>] = <span class="string">&quot;xxxxxxx&quot;</span>;  </span><br><span class="line">    paras[<span class="string">&quot;time_stamp&quot;</span>] = to_string(time(<span class="number">0</span>));  </span><br><span class="line">    paras[<span class="string">&quot;nonce_str&quot;</span>] = randstr();  </span><br><span class="line">    paras[<span class="string">&quot;session&quot;</span>] = <span class="string">&quot;10000&quot;</span>;  </span><br><span class="line">    paras[<span class="string">&quot;question&quot;</span>] = <span class="string">&quot;111&quot;</span>;  </span><br><span class="line">    </span><br><span class="line">map&lt;string, string&gt; getSign(map&lt;string,string&gt; paras, string appkey) &#123;  </span><br><span class="line">    <span class="built_in">string</span> before_md5 = <span class="string">&quot;&quot;</span>;  </span><br><span class="line">    Encoder encoder;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> iter = paras.begin(); iter != paras.end();iter++) &#123;  </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; iter-&gt;first &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; iter-&gt;second &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">        before_md5 += iter-&gt;first + <span class="string">&quot;=&quot;</span> + encoder.UTF8UrlEncode(iter-&gt;second) + <span class="string">&quot;&amp;&quot;</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    </span><br><span class="line">    before_md5 += <span class="string">&quot;app_key=&quot;</span> + appkey;  </span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; before_md5 &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">    MD5 sign;  </span><br><span class="line">    sign.update(before_md5);  </span><br><span class="line">    <span class="built_in">string</span> upper_sign = sign.toString();  </span><br><span class="line">    transform(upper_sign.begin(), upper_sign.end(), upper_sign.begin(), ::<span class="built_in">toupper</span>);  </span><br><span class="line">    paras[<span class="string">&quot;sign&quot;</span>] = upper_sign;  </span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; upper_sign &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">    <span class="keyword">return</span> paras;  </span><br><span class="line">&#125;  </span><br><span class="line">    <span class="built_in">string</span> appkey = <span class="string">&quot;xxxxxx&quot;</span>;  </span><br><span class="line">    paras = getSign(paras,appkey);  </span><br><span class="line">                          </span><br><span class="line">    Encoder encoder;  </span><br><span class="line">    <span class="built_in">string</span> http_query = <span class="string">&quot;&quot;</span>;  </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> iter = paras.begin(); iter != paras.end(); iter++) &#123;  </span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; iter-&gt;first &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; iter-&gt;second &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">        http_query += iter-&gt;first + <span class="string">&quot;=&quot;</span> + encoder.UTF8UrlEncode(iter-&gt;second) + <span class="string">&quot;&amp;&quot;</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    http_query.pop_back();  </span><br><span class="line">    ofile.open(<span class="string">&quot;CQ_question&quot;</span>, ios::out);  </span><br><span class="line">    ofile &lt;&lt; <span class="string">&quot;question&quot;</span> &lt;&lt; <span class="built_in">endl</span>;  </span><br><span class="line">    ofile.close();  </span><br><span class="line">    system(<span class="string">&quot;python CQ.py&quot;</span>);  </span><br><span class="line">    ifstream ifs;  </span><br><span class="line">    <span class="built_in">string</span> buf=<span class="string">&quot;&quot;</span>;  </span><br><span class="line">    _sleep(<span class="number">1000</span>);  </span><br><span class="line">    <span class="keyword">do</span> &#123;  </span><br><span class="line">        ifs.open(<span class="string">&quot;CQ_answer&quot;</span>, ios::in);  </span><br><span class="line">            </span><br><span class="line">        getline(ifs, buf);  </span><br><span class="line">        <span class="keyword">if</span> (buf.size() &gt; <span class="number">1</span>) &#123;  </span><br><span class="line">            logging::info_success(<span class="string">&quot;腾讯AI&quot;</span>, <span class="string">&quot;收到消息: &quot;</span> + buf);  </span><br><span class="line">            <span class="keyword">break</span>;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125; <span class="keyword">while</span> (<span class="literal">true</span>);  </span><br><span class="line"></span><br><span class="line">    send_message(e.target, MessageSegment::face(<span class="number">111</span>) + buf); <span class="comment">// 使用 message 模块构造消息  </span></span><br></pre></td></tr></table></figure><h2 id="参考Python版本"><a href="#参考Python版本" class="headerlink" title="参考Python版本"></a>参考<a href="https://blog.csdn.net/sily75/article/details/79300791">Python版本</a></h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;基于酷Q开发的QQ聊天机器人&quot;&gt;&lt;a href=&quot;#基于酷Q开发的QQ聊天机器人&quot; class=&quot;headerlink&quot; title=&quot;基于酷Q开发的QQ聊天机器人&quot;&gt;&lt;/a&gt;基于酷Q开发的QQ聊天机器人&lt;/h1&gt;&lt;p&gt;接入腾讯AI开放平台需要计算资源较好以及网络</summary>
      
    
    
    
    
    <category term="AI" scheme="https://learner0x5a.github.io/tags/AI/"/>
    
    <category term="C++" scheme="https://learner0x5a.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hexo-github 搭建网站</title>
    <link href="https://learner0x5a.github.io/2020/12/14/hexo-github-%E6%90%AD%E5%BB%BA%E7%BD%91%E7%AB%99/"/>
    <id>https://learner0x5a.github.io/2020/12/14/hexo-github-%E6%90%AD%E5%BB%BA%E7%BD%91%E7%AB%99/</id>
    <published>2020-12-14T11:34:12.000Z</published>
    <updated>2020-12-15T03:13:05.883Z</updated>
    
    <content type="html"><![CDATA[<h2 id="预备"><a href="#预备" class="headerlink" title="预备"></a>预备</h2><h3 id="安装git和nodejs"><a href="#安装git和nodejs" class="headerlink" title="安装git和nodejs"></a>安装git和nodejs</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install git nodejs npm</span><br></pre></td></tr></table></figure><h3 id="新建网站目录，例如test，安装hexo"><a href="#新建网站目录，例如test，安装hexo" class="headerlink" title="新建网站目录，例如test，安装hexo"></a>新建网站目录，例如test，安装hexo</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span>  </span><br><span class="line">sudo npm install -g hexo-cli  </span><br></pre></td></tr></table></figure><h2 id="搭建网站"><a href="#搭建网站" class="headerlink" title="搭建网站"></a>搭建网站</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo init <span class="built_in">test</span>  </span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span>   </span><br><span class="line">npm install  </span><br><span class="line"><span class="comment"># test文件夹下_config.yml为网站配置文件  </span></span><br></pre></td></tr></table></figure><h3 id="配置github"><a href="#配置github" class="headerlink" title="配置github"></a>配置github</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建一个名为 username.github.io的repo，username为github账户的用户名  </span></span><br><span class="line"><span class="comment"># 将这个repo的ssh链接填到_config.yml中，即修改deploy字段：  </span></span><br><span class="line"><span class="comment"># deploy:  </span></span><br><span class="line"><span class="comment">#   type: git  </span></span><br><span class="line"><span class="comment">#    repository: git@github.com:username/username.github.io.git  </span></span><br><span class="line"><span class="comment">#    branch: master  </span></span><br><span class="line"><span class="comment"># 然后  </span></span><br><span class="line">npm install hexo-deployer-git --save  </span><br></pre></td></tr></table></figure><h2 id="发布-更新网站"><a href="#发布-更新网站" class="headerlink" title="发布/更新网站"></a>发布/更新网站</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy  </span><br></pre></td></tr></table></figure><h2 id="配置主题"><a href="#配置主题" class="headerlink" title="配置主题"></a>配置<a href="https://hexo.io/themes/">主题</a></h2><p>我用了<a href="https://github.com/fi3ework/hexo-theme-archer">Archer</a>主题</p><h2 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h2><p>在source/下新建images目录，在md中用Markdown语法引用，例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![鄂A0260W](/images/red_cross.jpg)  </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;预备&quot;&gt;&lt;a href=&quot;#预备&quot; class=&quot;headerlink&quot; title=&quot;预备&quot;&gt;&lt;/a&gt;预备&lt;/h2&gt;&lt;h3 id=&quot;安装git和nodejs&quot;&gt;&lt;a href=&quot;#安装git和nodejs&quot; class=&quot;headerlink&quot; title=&quot;安</summary>
      
    
    
    
    
    <category term="Manual" scheme="https://learner0x5a.github.io/tags/Manual/"/>
    
  </entry>
  
  <entry>
    <title>2020 新年记</title>
    <link href="https://learner0x5a.github.io/2020/01/01/2020-new-year-note/"/>
    <id>https://learner0x5a.github.io/2020/01/01/2020-new-year-note/</id>
    <published>2020-01-01T11:37:40.000Z</published>
    <updated>2021-01-06T09:14:59.356Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2020-新年记"><a href="#2020-新年记" class="headerlink" title="2020 新年记"></a>2020 新年记</h1><p><img src="/images/0208.jpg" alt="相信"></p><p>朋友 在中国 在你身边 在这个特殊的时期 你看到了什么 又记住了什么 你为什么感动 又为什么彻夜难眠</p><p>2020.1.1</p><pre><code>    * 关于武汉市肺炎疫情的情况, **8名散布谣言者** 已被依法查处    * **华南海鲜市场** 关停</code></pre><p>2020.1.2</p><pre><code>    * 香港检视针对武汉疫情的预防措施</code></pre><p>2020.1.3</p><pre><code>    * 武汉卫健委通报44例，表示未见明显人传人和医护人员感染    * 李文亮签署 **训诫书**</code></pre><p>2020.1.4</p><pre><code>    * 武汉卫健委当日无通报</code></pre><p>2020.1.5</p><pre><code>    * 武汉卫健委通报44例，表示未见明显人传人和医护人员感染</code></pre><p>2020.1.6</p><pre><code>    * 武汉 **两会** 开幕    * 武汉卫健委当日无通报</code></pre><p>2020.1.7</p><pre><code>    * 武汉两会    * 武汉卫健委当日无通报</code></pre><p>2020.1.8</p><pre><code>    * 武汉两会    * 武汉卫健委当日无通报    * 国家卫健委确认新冠病毒为疫情病源</code></pre><p>2020.1.9</p><pre><code>    * 武汉两会    * 武汉卫健委当日无通报</code></pre><p>2020.1.10</p><pre><code>    * 武汉两会闭幕    * 武汉卫健委当日无通报    * **王广发** 称无医护人员感染</code></pre><p>2020.1.11</p><pre><code>    * 湖北省两会开幕    * 武汉卫健委通报41例，表示未见明显人传人和医护人员感染</code></pre><p>2020.1.12</p><pre><code>    * 湖北省两会    * 武汉卫健委当日无通报</code></pre><p>2020.1.13</p><pre><code>    * 湖北省两会    * 武汉卫健委当日通报无新增</code></pre><p>2020.1.14</p><pre><code>    * 湖北省两会    * 武汉卫健委当日通报无新增，表示未见明显人传人，不排除有限人传人</code></pre><p>2020.1.15</p><pre><code>    * 湖北省两会    * 武汉卫健委当日通报无新增    * 国家卫健委启动一级应急响应</code></pre><p>2020.1.16</p><pre><code>    * 湖北省两会    * 武汉卫健委当日通报无新增</code></pre><p>2020.1.17</p><pre><code>    * 故宫回应 **大G**    * 武汉死亡一例    * 武汉卫健委当日通报无新增    * 湖北省两会</code></pre><p><img src="/images/0117benz.jpg" alt="大G"></p><p>2020.1.18</p><pre><code>    * 湖北省两会闭幕    * 武汉卫健委恢复通报</code></pre><p>2020.1.19</p><pre><code>    * 武汉 **万家宴**</code></pre><p>2020.1.20</p><pre><code>    * 朝阳医院伤医事件， **陶勇** 医生受伤    * 钟南山确认人传人</code></pre><p>2020.1.21</p><pre><code>    * 新冠肺炎得到普遍报道，外交部回应</code></pre><p>2020.1.22</p><pre><code>    * 武汉对进出武汉人员管控</code></pre><p>2020.1.23</p><pre><code>    * 武汉 **封城**</code></pre><p><img src="/images/0123letter.jpg" alt="请战书"></p><p>2020.1.24</p><pre><code>    * 除夕，开心麻花团队表演小品《走过场》    * **台湾** 禁止口罩出口</code></pre><p>2020.1.25</p><pre><code>    * 陶勇医生苏醒，表示做不了手术可以做研究</code></pre><p>2020.1.26</p><pre><code>    * 湖北省长表示 **痛心内疚自责**    * 武汉市长表示武汉确诊病例可能再增加约 **1000例**    * 武汉市蔡甸区城管以 **车身不洁** 为由将一辆火神山建设车辆锁车</code></pre><p>2020.1.27</p><pre><code>    * **科比** 直升机失事    * 曝华南海鲜市场法人为 **余甜**</code></pre><p>2020.1.28</p><pre><code>    * 无症状患者确定也会传染</code></pre><p><img src="/images/0128slogan.jpg" alt="标语"></p><p>2020.1.29</p><pre><code>    * 西藏通报首例新型肺炎疑似病例    * **湖北省长** 感谢网友监督批评    * 由中国疾病预防控制中心、湖北省疾控中心等多单位共同完成的论文发表于SCI医学期刊《新英格兰医学杂志》    * 被隔离疑似患者鄢小文的儿子鄢成去世</code></pre><p>2020.1.30</p><pre><code>    * **黄冈卫健委主任** 唐志红被免职    * 岳父病毒性肺炎去世，女婿殴打医生，并导致医生的口罩、防护服被扯坏    * 湖北省 **红十字会** 向莆田系医院分配大量物资</code></pre><p>2020.1.31</p><pre><code>    * 武汉红十字会拦扣医疗物资    * 曝湖北红十字会会长是湖北省副省长赵海山    * 中国科学院上海药物研究所表示 **双黄连** 可以抑制新冠病毒    * 世界卫生组织正式宣布此次疫情为 **国际关注的公共卫生紧急事件**</code></pre><p>2020.2.1</p><pre><code>    * 武汉红十字会保安 **拦央视记者**    * **鄂A0260W**    * 湖南禽流感</code></pre><p><img src="/images/0201red_cross.jpg" alt="鄂A0260W"></p><p>2020.2.2</p><pre><code>    * 曝 **武汉病毒所所长** 裙带关系    * 火神山医院完工</code></pre><p>2020.2.3</p><pre><code>    * 清华大学全校师生同上一堂课    * **雨阔糖**    * 复工</code></pre><p>2020.2.4</p><pre><code>    * 武汉建立 **方舱医院**</code></pre><p>2020.2.5</p><pre><code>    * 武汉出现新生儿病例</code></pre><p>2020.2.6</p><pre><code>    * 曝 **大理** 征用重庆物资</code></pre><p><img src="/images/0206.jpg" alt="无理"></p><p>2020.2.7</p><pre><code>    * **李文亮** 医生去世    * 16省份 **一省包一市** 支援湖北</code></pre><p>2020.2.8</p><pre><code>    * 元宵节； **国家监委调查组** 抵达武汉</code></pre><p>2020.2.9</p><pre><code>    * 新型冠状病毒属于SARS病毒系口误</code></pre><p>2020.2.10</p><pre><code>    * 四川禽流感</code></pre><p>2020.2.11</p><pre><code>    * 湖北省卫健委书记主任被免职</code></pre><p>2020.2.12</p><pre><code>    * 一条 **劳力士** 的回家路</code></pre><p>2020.2.13</p><pre><code>    * 湖北省委书记调整    * 武汉市委书记调整</code></pre><p>2020.2.14</p><pre><code>    * 生物安全纳入 **国家安全体系**    * 全国医务人员确诊1716例</code></pre><p>2020.2.15</p><pre><code>    * 非洲大陆首例确诊    * 张家界疾控中心科长躲避疫情</code></pre><p>2020.2.16</p><pre><code>    * 何昊 **大意失荆州**</code></pre><p>2020.2.17</p><pre><code>    * 全国第一例新冠肺炎逝世患者遗体解剖    * 湖北省全面交通管制</code></pre><p>2020.2.18</p><pre><code>    * 武汉武昌医院院长刘智明感染新冠肺炎去世    * **江山娇**</code></pre><p>2020.2.19</p><pre><code>    * 武汉病毒所致信：问心无愧</code></pre><p>2020.2.20</p><pre><code>    * 孙小果被执行死刑</code></pre><p>2020.2.21</p><pre><code>    * 山东任城监狱爆发疫情    * 浙江十里丰监狱爆发疫情    * 湖北监狱系统爆发疫情</code></pre><p>2020.2.22</p><pre><code>    * 中科院：华南海鲜市场并非新冠病毒发源地</code></pre><p>2020.2.23</p><pre><code>    * 南医大28年遗案告破</code></pre><p>2020.2.24</p><pre><code>    * 大理市委书记高志宏被免职，大理市委副书记、市长杜淑敢被撤职，大理市政府党组成员、副市长娄增辉被撤职    * 全国人大常委会通过关于全面禁止非法野生动物交易的决定</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;2020-新年记&quot;&gt;&lt;a href=&quot;#2020-新年记&quot; class=&quot;headerlink&quot; title=&quot;2020 新年记&quot;&gt;&lt;/a&gt;2020 新年记&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/images/0208.jpg&quot; alt=&quot;相信&quot;&gt;&lt;/p&gt;
&lt;p&gt;朋</summary>
      
    
    
    
    
    <category term="others" scheme="https://learner0x5a.github.io/tags/others/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://learner0x5a.github.io/2020/01/01/hello-world/"/>
    <id>https://learner0x5a.github.io/2020/01/01/hello-world/</id>
    <published>2019-12-31T16:00:00.000Z</published>
    <updated>2021-01-06T09:15:41.114Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/zh-cn/docs/writing">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim _config.yml</span><br></pre></td></tr></table></figure><h3 id="Theme-Configuration"><a href="#Theme-Configuration" class="headerlink" title="Theme Configuration"></a>Theme Configuration</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim theme/archer/_config.yml</span><br></pre></td></tr></table></figure><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Update-website"><a href="#Update-website" class="headerlink" title="Update website"></a>Update website</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy</span><br></pre></td></tr></table></figure><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/zh-cn/docs/writing&quot;&gt;document</summary>
      
    
    
    
    
    <category term="Manual" scheme="https://learner0x5a.github.io/tags/Manual/"/>
    
  </entry>
  
</feed>
